# https://taskfile.dev

version: '3'

vars:
  BLAZEGRAPH_ENDPOINT: http://blazegraph:8080/blazegraph/sparql
  BLAZEGRAPH_ENDPOINT_SECONDARY: http://blazegraph-secondary:8080/blazegraph/sparql
  BLAZEGRAPH_BACKUP: http://blazegraph:8080/blazegraph/backup
  X3ML_ENDPOINT: http://localhost:8089/transform
  GENERATOR_POLICY: /mapping/generator-policy.xml
  NAMESPACE: https://data.skkg.ch/
  OUTPUTFOLDER_DUMP: /data/ttl/dump

output: 'prefixed'

tasks:

  default:
    desc: Runs the entire pipeline
    cmds:
      - '[ $(pgrep -cx "task") -gt 1 ] && echo "Another task is currently being executed. Wait for it to complete before running the pipeline." && exit 1 || echo "Running pipeline..." && exit 0'
      - task: download-source-items
      - task: prepare-and-perform-mapping-for-items
      - task: ingest-items
      - task: ingest-classifications
      - task: ingest-ontologies
      - task: ingest-alignments
      - task: retrieve-and-ingest-additional-data

  create-blazegraph-backup:
    desc: Creates a compressed Blazegraph backup
    vars:
      FILENAME:
        sh: echo "/backup/blazegraph-$(date "+%Y-%m-%d-%H-%M-%S").jnl.gz"
    cmds:
       - curl --data-urlencode "file={{.FILENAME}}" --data-urlencode "compress=true" --data-urlencode "block=true" {{.BLAZEGRAPH_BACKUP}}  

  create-data-dump:
    desc: Creates a TTL dump of the data generated by the pipeline
    vars:
      FILENAME:
        sh: echo dump-$(date "+%Y-%m-%d-%H-%M-%S")
    cmds:
      - mkdir -p {{.OUTPUTFOLDER_DUMP}}
      - rm -f {{.OUTPUTFOLDER_DUMP}}/*.ttl
      - find /data/ttl/main/ -name "*.ttl" -exec cat {} + > {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - task: _clean_turtle_file_from_extra_prefixes
        vars:
          FILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - gzip -c {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl > {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz
      - rm -f {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - # Remove old dumps if NUMBER_OF_DUMPS_TO_KEEP is set
      - if [ -n "$NUMBER_OF_DUMPS_TO_KEEP" ]; then
          ls -t {{.OUTPUTFOLDER_DUMP}}/dump-*.ttl.gz | tail -n +$(($NUMBER_OF_DUMPS_TO_KEEP+1)) | xargs rm -f;
        fi
      - echo "Data dump created at {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz"

  download-source-items:
    desc: Downloads all item records from MuseumPlus
    cmds:
      - for:
          - Address
          - Exhibition
          - Literature
          - Multimedia
          - Object
          - Person
          - Registrar
        task: _download-module-items-from-museumplus
        vars:
          MODULE: "{{.ITEM}}"

  download-source-vocabularies:
    desc: Downloads all vocabularies from MuseumPlus
    cmds:
      - for:
          - AdrCountryVgr
          - AdrTypeVgr
          - ExhExternalInternalVgr
          - ExhOrganiserTypeVgr
          - ExhTypeVgr
          - ExhStatusVgr
          - GenCurrencyVgr
          - GenLanguageVgr
          - GenMoveStatusVgr
          - LitAuthorTypeVgr
          - LitLanguageVgr
          - LitTypeVgr
          - LitPlaceVgr
          - LitPersonTypeVgr
          - MulRightsTypeVgr
          - MulUsageVgr
          - MulTypeVgr
          - ObjAcquisitionModeVgr
          - ObjCategoryVgr
          - ObjConditionShortVgr
          - ObjDatePeriodVgr
          - ObjDatePrefixVgr
          - ObjDateSourceVgr
          - ObjDateSuffixVgr
          - ObjDateTypeVgr
          - ObjDim3DTypeVgr
          - ObjDocumentationStatusVgr
          - ObjIconographyVgr
          - ObjIconographyTypeVgr
          - ObjInscriptionDialogTypeVgr
          - ObjInscriptionTypeVgr
          - ObjInscriptionPositionVgr
          - ObjMaterialVgr
          - ObjMaterialTechniqueVgr
          - ObjMaterialTechniqueTypeVgr
          - ObjObjectTitleTypeVgr
          - ObjObjectTitleSourceVgr
          - ObjObjectTypeVgr
          - ObjOtherNumberTypeVgr
          - ObjPerAssociationRoleVgr
          - ObjPerAssociationAttributionVgr
          - ObjTechniqueVgr
          - PerDateTypeVgr
          - PerGenderVgr
          - PerTypeVgr
        task: _download-vocabulary-from-museumplus
        vars:
          VOCABULARY: "{{.ITEM}}"
          
  download-items-for-module:
    desc: Downloads the item records for a specific module from MuseumPlus. Pass the module name as command line argument
    interactive: True
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "{{.CLI_ARGS}}"
          else
            echo "{{.MODULE}}"
          fi
      MODULE_UCFIRST:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print toupper(substr($0,0,1)) tolower(substr($0,2))}')"
    cmds:
      - task: _download-module-items-from-museumplus
        vars:
          MODULE: "{{.MODULE_UCFIRST}}"

  execute-query-from-file:
    desc: Execute a query against the main Blazegraph instance. Pass the path to the file as command line argument
    cmds:
      - |
        curl -X POST -H "Accept: text/csv" --data-urlencode "query=$(cat "{{.CLI_ARGS}}")" {{.BLAZEGRAPH_ENDPOINT}}

  first-run:
    desc: Task to run when the pipeline is run for the first time
    cmds:
      - task: update-vocabularies
      - task: update-iiif 
      - task: ingest-platform-data

  generate-example-record-address:
    desc: Generates an example Address record for developing the mapping in the X3ML editor
    vars:
      OUTPUTFILE: /mapping/example-address-record.xml
    cmds:
      - task: _generate-example-record
        vars:
          OUTPUTFILE: "{{.OUTPUTFILE}}"
          MODULE: Address
          INPUTFILES: /data/source/address/address-item-260a0a70-4d26-4f15-8b27-d87243c47c24.xml /data/source/address/address-item-0df1724d-e9e8-4b0c-96db-bc1ee4696000.xml /data/source/address/address-item-0b5918d4-c549-4477-8c82-159314e3b525.xml /data/source/address/address-item-3a7892e0-9d41-4561-9595-42070e0b9e66.xml /data/source/address/address-item-287ee736-19da-4806-a722-c3e389ab8c4a.xml /data/source/address/address-item-0a4ea0e2-f98f-4966-a0df-ab159620d3bf.xml

  generate-example-record-exhibition:
    desc: Generates an example Exhibition record for developing the mapping in the X3ML editor
    vars:
      OUTPUTFILE: /mapping/example-exhibition-record.xml
    cmds:
      - task: _generate-example-record
        vars:
          OUTPUTFILE: "{{.OUTPUTFILE}}"
          MODULE: Exhibition
          INPUTFILES: /data/source/exhibition/exhibition-item-53b63629-5c82-4614-aecc-7bf3075778a7.xml /data/source/exhibition/exhibition-item-4.xml
  
  generate-example-record-object:
    desc: Generates an example Object record for developing the mapping in the X3ML editor
    vars:
      OUTPUTFILE: /mapping/example-object-record.xml
    cmds:
      - task: _generate-example-record
        vars:
          OUTPUTFILE: "{{.OUTPUTFILE}}"
          MODULE: Object
          INPUTFILES: /data/source/object/object-item-91aa0c4a-3325-45f6-9f29-e90eb13ec4c2.xml /data/source/object/object-item-9414327b-74ab-4425-b123-ecbb28db5168.xml /data/source/object/object-item-305.xml /data/source/object/object-item-10094.xml /data/source/object/object-item-857.xml /data/source/object/object-item-493.xml /data/source/object/object-item-4f66e941-1bcb-4e14-baab-ffdaef1c3436.xml /data/source/object/object-item-0a5b480f-04d5-4053-840a-5185a2abda39.xml /data/source/object/object-item-c6b149c2-f809-4d8c-8dc9-70acb24e0070.xml /data/source/object/object-item-3412c1a2-3c87-462a-8943-34aede5dcf5a.xml /data/source/object/object-item-2a0c8fab-5771-496c-9350-5482f50c93e2.xml /data/source/object/object-item-0a5b9c6a-2d1d-43c5-b833-6975ddfa7ded.xml /data/source/object/object-item-3c7215d0-3aba-4bb8-98e6-5da239ab0a64.xml /data/source/object/object-item-0b17827d-117b-42b7-9925-0731f6db779b.xml /data/source/object/object-item-0a0c96ab-71f1-466a-a5ff-ab556d437f15.xml /data/source/object/object-item-ffa356cf-f988-4d96-9580-92dcb0e4dddf.xml
  
  generate-example-record-literature:
    desc: Generates an example Literature record for developing the mapping in the X3ML editor
    vars:
      OUTPUTFILE: /mapping/example-literature-record.xml
    cmds:
      - task: _generate-example-record
        vars:
          OUTPUTFILE: "{{.OUTPUTFILE}}"
          MODULE: Literature
          INPUTFILES: /data/source/literature/literature-item-b16e2d5f-af15-4a6d-99be-e06af532923e.xml /data/source/literature/literature-item-a50e1a84-367c-4a9f-8357-ebc4fe29badc.xml /data/source/literature/literature-item-710a0732-993d-4d4e-8611-5fe1714c2385.xml /data/source/literature/literature-item-0b6b164a-d7d2-4c58-8f94-9dcf2eb6a9dc.xml /data/source/literature/literature-item-a6fda927-827b-4824-8cfc-2f91dcea74f8.xml /data/source/literature/literature-item-118.xml /data/source/literature/literature-item-0a1ab550-dabb-4afd-be17-f0f43725be33.xml /data/source/literature/literature-item-0b7ee850-6fd8-45ba-af19-267548c9c4cf.xml
  
  generate-example-record-multimedia:
    desc: Generates an example Multimedia record for developing the mapping in the X3ML editor
    vars:
      OUTPUTFILE: /mapping/example-multimedia-record.xml
    cmds:
      - task: _generate-example-record
        vars:
          OUTPUTFILE: "{{.OUTPUTFILE}}"
          MODULE: Multimedia
          INPUTFILES: /data/source/multimedia/multimedia-item-d2069a25-3b97-4bf3-95ca-85f1330cf9f3.xml /data/source/multimedia/multimedia-item-0eac7e95-8ff3-460d-a75b-37f66b717fc7.xml /data/source/multimedia/multimedia-item-211a5e70-5474-4019-8689-4791a989cd8f.xml /data/source/multimedia/multimedia-item-0e137c8d-d712-4e08-9d6c-9776d25f651f.xml

  generate-example-record-person:
    desc: Generates an example Person record for developing the mapping in the X3ML editor
    vars:
      OUTPUTFILE: /mapping/example-person-record.xml
    cmds:
      - task: _generate-example-record
        vars:
          OUTPUTFILE: "{{.OUTPUTFILE}}"
          MODULE: Person
          INPUTFILES: /data/source/person/person-item-8bf5f0fb-aeb4-4f83-8308-6df89863a1ec.xml /data/source/person/person-item-0b159fe6-f699-45b3-8e61-b7377856d8b7.xml /data/source/person/person-item-1111.xml /data/source/person/person-item-88c3cca8-a587-44b8-8c6d-194dd285b1dd.xml /data/source/person/person-item-0d2611a7-961e-420a-8b04-dc628c3f70fb.xml /data/source/person/person-item-f447a4d2-7be1-4d7c-8dcb-64381b20a9bc.xml

  generate-example-record-registrar:
    desc: Generates an example Registrar record for developing the mapping in the X3ML editor
    vars:
      OUTPUTFILE: /mapping/example-registrar-record.xml
    cmds:
      - task: _generate-example-record
        vars:
          OUTPUTFILE: "{{.OUTPUTFILE}}"
          MODULE: Registrar
          INPUTFILES: /data/source/registrar/registrar-item-dabea9ee-8e3a-435d-a4f6-ca38a8b0bc51.xml

  generate-mapping-file-from-3m:
    desc: Generates a mapping file from data stored in the 3M Editor.
    cmds:
      - python /scripts/retrieveMappingFromExist.py {{.CLI_ARGS}}
  
  generate-field-definitions:
    desc: Generates the field definitions for the platform based on the fieldDefinitions.yml file
    sources:
      - /apps/skkg/src/fieldDefinitions.yml
    vars:
      INPUTFILE: /apps/skkg/src/fieldDefinitions.yml
      JSONOUTPUT: /apps/skkg/data/templates/https%3A%2F%2Fstatic.swissartresearch.net%2Fpartial%2FfieldDefinitions.html
      INLINEOUTPUT: /apps/skkg/data/templates/https%3A%2F%2Fstatic.swissartresearch.net%2Fpartial%2FfieldDefinitionsInline.html
    cmds:
      - semantic-field-util -f JSON -y {{.INPUTFILE}}  write -t {{.JSONOUTPUT}}
      - semantic-field-util -f INLINE -y {{.INPUTFILE}}  write -t {{.INLINEOUTPUT}}
  
  ingest-alignments:
    desc: Ingests the alignments into the triplestore
    sources:
      - /data/ttl/additional/alignments/*.ttl
    cmds:
      - for: sources
        task: _ingest-data-from-file
        vars:
          NAME:
            sh: echo "$(basename {{.ITEM}} .ttl)"
          FILE: "{{.ITEM}}"
          TYPE: application/x-turtle
          GRAPH: 
            sh: echo "{{.NAMESPACE}}graph/alignments/$(basename {{.ITEM}} .ttl)"
          ENDPOINT: "{{.BLAZEGRAPH_ENDPOINT}}"

  ingest-classifications:
    desc: Ingest classifications into the triplestore
    sources:
      - /data/ttl/additional/classifications/*.ttl
    cmds:
      - for: sources
        task: _ingest-data-from-file
        vars:
          NAME:
            sh: echo "$(basename {{.ITEM}} .ttl)"
          FILE: "{{.ITEM}}"
          TYPE: application/x-turtle
          GRAPH: 
            sh: echo "{{.NAMESPACE}}graph/classifications/$(basename {{.ITEM}} .ttl)"
          ENDPOINT: "{{.BLAZEGRAPH_ENDPOINT_SECONDARY}}"
  
  ingest-items:
    desc: Ingest items for all modules. Add --debug true To see the response from the triplestore
    cmds:
      - for:
        - object
        - address
        - person
        - multimedia
        - literature
        - exhibition
        - registrar
        task: _ingest_module_items
        vars:
          MODULE: "{{.ITEM}}"

  ingest-iiif:
    desc: Ingest IIIF data into the triplestore
    sources:
      - /data/ttl/main/iiif/*.ttl
    cmds:
      - for: sources
        task: _drop-graph
        vars:
          GRAPH:
            sh: echo "{{.NAMESPACE}}graph/iiif/$(basename {{.ITEM}} .ttl)"
      - for: sources
        task: _ingest-data-from-file
        vars:
          NAME:
            sh: echo "$(basename {{.ITEM}} .ttl)"
          FILE: "{{.ITEM}}"
          TYPE: application/x-turtle
          GRAPH: 
            sh: echo "{{.NAMESPACE}}graph/iiif/$(basename {{.ITEM}} .ttl)"
          ENDPOINT: "{{.BLAZEGRAPH_ENDPOINT}}"

  ingest-module-items:
    desc: Ingests the items for a specific module. The module name should be passed as an argument or via the MODULE variable.
    interactive: True
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "{{.CLI_ARGS}}"
          else
            echo "{{.MODULE}}"
          fi
      MODULE_UCFIRST:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print toupper(substr($0,0,1)) tolower(substr($0,2))}')"
    cmds:
      - task: _ingest_module_items
        vars:
          MODULE: "{{.MODULE_UCFIRST}}"
          DEBUG: true
  
  ingest-ontologies:
    desc: Ingests the ontologies into individual named Graphs
    sources:
      - /mapping/schemas/*.*
    cmds:
      - task: _ingest-data-from-file
        vars:
          NAME: CIDOC-CRM
          FILE: /mapping/schemas/CIDOC_CRM_7.1.1_RDFS_Impl_v1.1.rdfs
          TYPE: application/rdf+xml
          GRAPH: http://www.cidoc-crm.org/cidoc-crm
      - task: _ingest-data-from-file
        vars:
          NAME: CIDOC-CRM PC
          FILE: /mapping/schemas/CIDOC_CRM_v7.1.3_PC.rdf
          TYPE: application/rdf+xml
          GRAPH: http://www.cidoc-crm.org/cidoc-crm/pc
      - task: _ingest-data-from-file
        vars:
          NAME: CRMdig
          FILE: /mapping/schemas/CRMdig_v3.2.1.rdfs
          TYPE: application/rdf+xml
          GRAPH: http://www.ics.forth.gr/isl/CRMdig
      - task: _ingest-data-from-file
        vars:
          NAME: CRMsci
          FILE: /mapping/schemas/CRMsci_v2.0.rdfs
          TYPE: application/rdf+xml
          GRAPH: http://www.cidoc-crm.org/extensions/crmsci
      - task: _ingest-data-from-file
        vars:
          NAME: FRBRoo
          FILE: /mapping/schemas/FRBR2.4.rdfs
          TYPE: application/rdf+xml
          GRAPH: http://iflastandards.info/ns/fr/frbr/frbroo
      - task: _ingest-data-from-file
        vars:
          NAME: Linked.Art
          FILE: /mapping/schemas/LinkedArt.rdfs
          TYPE: application/rdf+xml
          GRAPH: https://linked.art/ns/terms
      - task: _ingest-data-from-file
        vars:
          NAME: crmaaa
          FILE: /mapping/schemas/CRM_AAA_v1.5.4.ttl
          TYPE: application/x-turtle
          GRAPH: https://takin.solutions/ontologies/crmaaa
      - task: _ingest-data-from-file
        vars:
          NAME: ResearchSpace Ontology
          FILE: /mapping/schemas/rs-ontology.ttl
          TYPE: application/x-turtle
          GRAPH: http://www.researchspace.org/ontology
      - task: _ingest-data-from-file
        vars:
          NAME: SKKG Ontology
          FILE: /mapping/schemas/skkg-ontology.ttl
          TYPE: application/x-turtle
          GRAPH: http://ontology.skkg.ch/ontology

  ingest-platform-data:
    desc: Ingests the data used for the operation of the platform
    sources:
      - /data/platform/*.trig
    cmds: 
      - for: sources
        task: _ingest-data-from-file
        vars:
          NAME: "{{.ITEM}}"
          FILE: "{{.ITEM}}"
          TYPE: application/x-trig

  ingest-vocabularies:
    - task: _ingest_vocabularies
      vars:
        INPUTFOLDER: /data/ttl/main/vocabularies
        NAMESPACE: https://data.skkg.ch/type/

  perform-mapping-for-module-items:
    desc: Performs the mapping for a specific module. The module name should be passed as an argument or via the MODULE variable.
    interactive: True
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "{{.CLI_ARGS}}"
          else
            echo "{{.MODULE}}"
          fi
      MODULE_UCFIRST:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print toupper(substr($0,0,1)) tolower(substr($0,2))}')"
    cmds:
      - task: _perform-mapping-for-module-items
        vars:
          MODULE: "{{.MODULE_UCFIRST}}"

  prepare-and-perform-mapping-for-items:
    desc: Prepares and performs the mapping for all modules
    cmds:
      - echo "Preparing mapping..."
      - for:
          - Address
          - Exhibition
          - Literature
          - Multimedia
          - Object
          - Person
          - Registrar
        task: _prepare-mapping-for-module-items
        vars:
          MODULE: "{{.ITEM}}"
      - echo "Mapping..."
      - for:
          - Address
          - Exhibition
          - Literature
          - Multimedia
          - Object
          - Person
        task: _perform-mapping-for-module-items
        vars:
          MODULE: "{{.ITEM}}"

  prepare-mapping-for-module-items:
    desc: Prepares the mapping for a specific module. The module name should be passed as an argument or via the MODULE variable.
    interactive: True
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "{{.CLI_ARGS}}"
          else
            echo "{{.MODULE}}"
          fi
      MODULE_UCFIRST:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print toupper(substr($0,0,1)) tolower(substr($0,2))}')"
    cmds:
      - task: _prepare-mapping-for-module-items
        vars:
          MODULE: "{{.MODULE_UCFIRST}}"
    
  prepare-and-perform-mapping-for-iiif:
    desc: Performs the mapping for the IIIF data
    sources:
      - /mapping/mapping-iiif.x3ml
      - /data/source/iiif/*.xml
    vars:
      SOURCEFOLDER: /data/source/iiif
      INPUTFOLDER: /mapping/input/iiif
      OUTPUTFOLDER: /data/ttl/main/iiif
      MAPPINGFILE: /mapping/mapping-iiif.x3ml
    cmds:
      - mkdir -p {{.INPUTFOLDER}}
      - mkdir -p {{.OUTPUTFOLDER}}
      - rm -rf {{.INPUTFOLDER}}/*.xml
      - rm -rf {{.OUTPUTFOLDER}}/*.ttl
      - # Copy all XML files from the source folder to the input folder
      - find {{.SOURCEFOLDER}} -maxdepth 1 -name "*.xml" -exec cp -t {{.INPUTFOLDER}} {} +
      - bash /scripts/performMapping.sh -i {{.INPUTFOLDER}} -o {{.OUTPUTFOLDER}} -m {{.MAPPINGFILE}} -g {{.GENERATOR_POLICY}} -j {{.X3ML_ENDPOINT}}
      - find {{.OUTPUTFOLDER}} -maxdepth 1 -name "*.ttl" -exec cat {} + > {{.OUTPUTFOLDER}}/iiifPaths.ttl.temp
      - find {{.OUTPUTFOLDER}} -maxdepth 1 -name "*.ttl" -delete
      - mv {{.OUTPUTFOLDER}}/iiifPaths.ttl.temp {{.OUTPUTFOLDER}}/iiifPaths.ttl
      - task: _clean_turtle_file_from_extra_prefixes
        vars:
          FILE: "{{.OUTPUTFOLDER}}/iiifPaths.ttl"

  perform-mapping-for-vocabularies:
    desc: Performs the mapping for the vocabularies
    sources:
      - /mapping/mapping-vocabulary.x3ml
      - /data/source/vocabularies/*.xml
    vars:
      SOURCEFOLDER: "/data/source/vocabularies"
      INPUTFOLDER: "/mapping/input/vocabularies"
      OUTPUTFOLDER: "/data/ttl/main/vocabularies"
      MAPPINGFILE: "/mapping/mapping-vocabulary.x3ml"
    cmds:
      - mkdir -p {{.INPUTFOLDER}}
      - mkdir -p {{.OUTPUTFOLDER}}
      - # Copy all XML files from the source folder to the input folder
      - find {{.SOURCEFOLDER}} -maxdepth 1 -name "*.xml" -exec cp -t {{.INPUTFOLDER}} {} +
      - # Remove the string 'xmlns="http://www.zetcom.com/ria/ws/vocabulary"' from the XML files in the input folder as X3ML is not able to handle XML namespaces
      - find {{.INPUTFOLDER}} -maxdepth 1 -name "*.xml" -exec sed -i 's/xmlns="http:\/\/www.zetcom.com\/ria\/ws\/vocabulary"//g' {} +
      - find {{.OUTPUTFOLDER}} -maxdepth 1 -name "*.ttl" -delete
      - bash /scripts/performMapping.sh -i {{.INPUTFOLDER}} -o {{.OUTPUTFOLDER}} -m {{.MAPPINGFILE}} -g {{.GENERATOR_POLICY}}  -j {{.X3ML_ENDPOINT}}

  push-latest-data-dump:
    desc: Upload latest data dump to S3 endpoint for data sharing
    vars:
      FILEPATH:
        sh: echo "$(ls -t {{.OUTPUTFOLDER_DUMP}}/*.ttl.gz | head -1)"
      FILENAME:
        sh: echo "$(basename {{.FILEPATH}})"
      LATEST_FILENAME: latest-dump.ttl.gz
    cmds:
      - echo "Uploading {{.FILEPATH}} to S3 bucket $S3_BUCKET_PUSH..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.FILENAME}}
      - echo "Uploading {{.FILEPATH}} as latest dump..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.LATEST_FILENAME}}
  
  recreate-folder-metadata:
    desc: Recreate the metadata for a specific module. The module name should be passed as an argument or via the MODULE variable.
    interactive: True
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "{{.CLI_ARGS}}"
          else
            echo "{{.MODULE}}"
          fi
    cmds:
      - rm -f /data/source/{{.MODULE}}/metadata.json
      - python /scripts/recreateMetadata.py --folder /data/source/{{.MODULE}}

  remove-unpublished-module-items:
    desc: Removes item records that have been unpublished from MuseumPlus for a specific module. The module name should be passed as an argument or via the MODULE variable.
    interactive: True
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "{{.CLI_ARGS}}"
          else
            echo "{{.MODULE}}"
          fi
      MODULE_UCFIRST: 
        sh: echo "$(echo "{{.MODULE}}" | awk '{print toupper(substr($0,0,1)) tolower(substr($0,2))}')"
    cmds:
      - echo "Removing unpublished {{.MODULE}} items..."
      - task: _remove-unpublished-module-items
        vars:
          MODULE: "{{.MODULE_UCFIRST}}"

  remove-unpublished-items:
    desc: Removes all item records that have been unpublished from MuseumPlus. Note that Multimedia items are skipped and should be processed separately if needed.
    cmds:
      - for:
          - Address
          - Exhibition
          - Literature
          - Object
          - Person
          - Registrar
        task: _remove-unpublished-module-items
        vars:
          MODULE: "{{.ITEM}}"

  remove-module-items-from-triplestore:
    desc: Removes all item records for a specific module from the triplestore. The module name should be passed as an argument or via the MODULE variable.
    prompt: This will remove all item records for the module {{.MODULE_UCFIRST}} from the triplestore. Do you want to continue?
    interactive: True
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "{{.CLI_ARGS}}"
          else
            echo "{{.MODULE}}"
          fi
      MODULE_UCFIRST:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print toupper(substr($0,0,1)) tolower(substr($0,2))}')"
    cmds:
      - task: _remove-module-items-from-triplestore
        vars:
          MODULE: "{{.MODULE_UCFIRST}}"
      - task: reset-last-ingested-metadata
        vars:
          MODULE: "{{.MODULE_UCFIRST}}"

  remove-items-without-equivalent-ttl-from-triplestore:
    desc: Removes all item records from the triplestore that do not have an equivalent TTL file. Note that Multimedia items are skipped and should be processed separately if needed.
    cmds:
      - for:
          - Address
          - Exhibition
          - Literature
          - Object
          - Person
        task: _remove-module-items-without-equivalent-ttl-from-triplestore
        vars:
          MODULE: "{{.ITEM}}"

  remove-module-items-without-equivalent-ttl-from-triplestore:
    desc: Removes all item records for a specific module from the triplestore that do not have an equivalent TTL file. The module name should be passed as an argument or via the MODULE variable.
    interactive: True
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "{{.CLI_ARGS}}"
          else
            echo "{{.MODULE}}"
          fi
      MODULE_UCFIRST:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print toupper(substr($0,0,1)) tolower(substr($0,2))}')"
    cmds:
      - task: _remove-module-items-without-equivalent-ttl-from-triplestore
        vars:
          MODULE: "{{.MODULE_UCFIRST}}"

  retrieve-and-ingest-additional-data:
    desc: Retrieve and ingest additional data for external URIs in the Triple Store 
    interactive: True
    vars:
      OUTPUTFOLDER: /data/ttl/additional/external
      PREDICATES: http://www.ics.forth.gr/isl/CRMdig/L54_is_same-as,http://www.cidoc-crm.org/cidoc-crm/P2_has_type,http://www.cidoc-crm.org/cidoc-crm/P127_has_broader_term,http://www.w3.org/2004/02/skos/core#exactMatch,http://www.w3.org/2004/02/skos/core#closeMatch,http://www.w3.org/2004/02/skos/core#relatedMatch,http://www.w3.org/2004/02/skos/core#related
      SOURCES: gnd,aat,lt
      INGEST: True
      INGESTUPDATE: True
      INGESTNAMESPACE: https://data.skkg.ch/graph/externalData/
    cmds:
      - python /scripts/retrieveAdditionalData.py --endpoint {{.BLAZEGRAPH_ENDPOINT}} --outputFolder {{.OUTPUTFOLDER}} --predicates "{{.PREDICATES}}" --ingest {{.INGEST}} --ingestNamespace {{.INGESTNAMESPACE}} --ingestUpdate {{.INGESTUPDATE}} --sources "{{.SOURCES}}"
      - for:
          - gnd
          - aat
          - lt
        task: _clean_turtle_file_from_extra_prefixes
        vars:
          FILE: "/data/ttl/additional/external/{{.ITEM}}.ttl"


  retrieve-iiif-data:
    desc: Downloads and prepares data related to the IIIF images
    vars:
      OUTPUTFOLDER: /data/source/iiif
    cmds:
      - mkdir -p {{.OUTPUTFOLDER}}
      - python /scripts/retrieveIiifData.py --input $IIIF_CSV_URL --outputFolder {{.OUTPUTFOLDER}} --filename iiifPaths
 
  reset:
    desc: Delete all artefacts produced by the pipeline.
    prompt: This will delete all artefacts produced by the pipeline... Do you want to continue?
    cmds:
      - rm -f /scripts/.task/checksum/*
      - for:
          - Address
          - Exhibition
          - Literature
          - Multimedia
          - Object
          - Person
        task: reset-module
        vars:
          MODULE: "{{.ITEM}}"
      - task: _reset-iiif-and-vocabularies
      - echo "Done!"

  reset-iiif:
    desc: Delete all artefacts produced by the pipeline for the iiif data.
    cmds: 
      - rm -rf /data/ttl/main/iiif
      - rm -rf /mapping/input/iiif
      - rm -rf /mapping/output/iiif

  reset-module:
    desc: Delete all artefacts produced by the pipeline for a given module. The module name should be passed as an argument or via the MODULE variable.
    silent: true
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "{{.CLI_ARGS}}"
          else
            echo "{{.MODULE}}"
          fi
      MODULE_LOWERCASE:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      MODULE_UCFIRST:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print toupper(substr($0,0,1)) tolower(substr($0,2))}')"
    cmds:
      - |
        if [ -z "{{.MODULE}}" ]; then
          echo "No module specified as variable or cli argument. Exiting script."
          exit 1
        fi
      - echo "Deleting files for {{.MODULE}}..."
      - rm -rf /data/temp/download/temp_{{.MODULE_UCFIRST}}
      - rm -rf /data/temp/ingest/{{.MODULE_LOWERCASE}}
      - rm -rf /data/ttl/main/{{.MODULE_LOWERCASE}}
      - rm -rf /mapping/input/{{.MODULE_LOWERCASE}}
      - rm -rf /mapping/output/{{.MODULE_LOWERCASE}}
      - echo "Resetting metadata for {{.MODULE}}.."
      - task: recreate-folder-metadata
        vars:
          MODULE: "{{.MODULE_LOWERCASE}}"

  reset-vocabularies:
    desc: Delete all artefacts produced by the pipeline for the vocabularies.
    cmds: 
      - rm -rf /data/ttl/main/vocabularies
      - rm -rf /mapping/input/vocabularies
      - rm -rf /mapping/output/vocabularies

  reset-last-ingested-metadata:
    desc: Resets the last ingested metadata for a specific module. Pass the module name as an argument and optionally a new date (YYYY-MM-DD).
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "$(echo "{{.CLI_ARGS}}" | awk '{print $1}')"
          else
            echo "{{.MODULE}}"
          fi
      DATE:
        sh: |
          ARG_DATE=$(echo "{{.CLI_ARGS}}" | awk '{print $2}')
          if [ -z "$ARG_DATE" ]; then
            echo "1970-01-01 00:00:00.000"
          else
            echo "$ARG_DATE 00:00:00.000"
          fi
      SOURCEFOLDER:
        sh: |
          echo "/data/source/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
    cmds:
      - task: _update-metadata-for-module
        vars:
          MODULE: "{{.MODULE}}"
          KEY: lastIngested
          VALUE: "{{.DATE}}"
          SOURCEFOLDER: "{{.SOURCEFOLDER}}"
          FILEEXTENSION: .xml

  reset-last-mapped-metadata:
    desc: Resets the last mapped metadata for a specific module. Pass the module name as an argument and optionally a new date (YYYY-MM-DD).
    vars:
      MODULE:
        sh: |
          if [ -z "{{.MODULE}}" ]; then
            echo "$(echo "{{.CLI_ARGS}}" | awk '{print $1}')"
          else
            echo "{{.MODULE}}"
          fi
      DATE:
        sh: |
          ARG_DATE=$(echo "{{.CLI_ARGS}}" | awk '{print $2}')
          if [ -z "$ARG_DATE" ]; then
            echo "1970-01-01 00:00:00.000"
          else
            echo "$ARG_DATE 00:00:00.000"
          fi
      SOURCEFOLDER:
        sh: |
          echo "/data/source/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
    cmds:
      - task: _update-metadata-for-module
        vars:
          MODULE: "{{.MODULE}}"
          KEY: lastMapped
          VALUE: "{{.DATE}}"
          SOURCEFOLDER: "{{.SOURCEFOLDER}}"
          FILEEXTENSION: .xml

  run-pipeline-cycles:
    desc: Runs the entire pipeline as well as certain tasks according to a specific interval. When this task is executed, each step of the pipeline will be run if the interval has passed since the last execution of the task. The interval is defined in the vars section of each task.
    cmds:
      - task: _run-task-according-to-interval
        vars:
          TASK: default
          INTERVAL: 1d
      - task:  _run-task-according-to-interval
        vars:
          TASK: update-vocabularies
          INTERVAL: 1w
      - task:  _run-task-according-to-interval
        vars:
          TASK: update-iiif
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: remove-unpublished-items
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: remove-items-without-equivalent-ttl-from-triplestore
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: create-data-dump
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: push-latest-data-dump
          INTERVAL: 1w
  
  show-last-pipeline-cycle-runs:
    desc: Displays the last pipeline cycle runs for each task
    cmds:
      - |
        DIRECTORY="/scripts/.task/lastrun"

        printf "%-30s %-20s\n" "Task" "Last Run"
        printf "%-30s %-20s\n" "----------------" "-----------------"

        for FILE in "$DIRECTORY"/*; do
          # Get the base name of the file (without the directory path)
          BASENAME=$(basename "$FILE")

          # Extract the timestamp from the file (adjust this command based on file content)
          TIMESTAMP=$(cat "$FILE" | grep -oE '[0-9]{10}' | head -n 1)
          
          if [[ -n "$TIMESTAMP" ]]; then
            # Convert the timestamp to a human-readable date
            HUMAN_DATE=$(date -d @"$TIMESTAMP" +"%Y-%m-%d %H:%M:%S")
            printf "%-30s %-20s\n" "$BASENAME" "$HUMAN_DATE"
          else
            printf "%-30s %-20s\n" "$BASENAME" "No valid timestamp"
          fi
        done
  
  suggest-alignments-for-vocabularies:
    desc: Suggest alignments for all vocabularies with GND data
    sources:
      - /data/ttl/main/vocabularies/*.ttl
    cmds:
      - for: sources
        task: _suggest-alignments
        vars:
          INPUT_FILE: "{{.ITEM}}"
          RECONCILIATION_TYPE: SubjectHeading
          BASE_TYPE: https://ontology.skkg.ch/Type
          LOG_FILE: /logs/suggest-alignments-for-vocabularies.log

  update-iiif:
    desc: Downloads, maps, and ingests the IIIF data
    cmds: 
      - task: retrieve-iiif-data
      - task: prepare-and-perform-mapping-for-iiif
      - task: ingest-iiif

  update-vocabularies:
    desc: Downloads, maps, and ingests the vocabularies
    cmds:
      - task: download-source-vocabularies
      - task: perform-mapping-for-vocabularies
      - sleep 5s
      - task: suggest-alignments-for-vocabularies
      - task: ingest-vocabularies
  
  validate-turtle-file:
    desc: Validate a Turtle file using SHACL. Pass the file to validate through command line argument
    cmds:
      - pyshacl -s /mapping/shapesGraph.ttl -e /mapping/schemas/skkg-ontology.ttl -e https://cidoc-crm.org/rdfs/7.1.1/CIDOC_CRM_v7.1.1.rdfs -m -i rdfs -a -j -f turtle {{.CLI_ARGS}}

  _combine_and_ingest_shacl_reports:
    internal: true
    requires:
      vars: [TEMP_FOLDER, REPORTS_GRAPH, REPORT_IRI]
    status:
      - '[ "$(find {{.TEMP_FOLDER}} -name "*.ttl" -type f)" ] || exit 0 && exit 1'
    vars:
      ENDPOINT: "{{.BLAZEGRAPH_ENDPOINT_SECONDARY}}"
    cmds:
      # Combine the failed tests into a single N-Triples file. We use N-Triples so we have a single line per triple
      - rm -f {{.TEMP_FOLDER}}/validation_output.nt
      - |
        find {{.TEMP_FOLDER}} -name '*.ttl' -exec sh -c '
          filename=$(basename "{}" .ttl)
          rapper -q -i turtle -o ntriples "{}" | sed "s/_:genid/_:genid_${filename}_/g" >> {{.TEMP_FOLDER}}/validation_output.nt
        ' \;
      - find {{.TEMP_FOLDER}} -type f -name '*.ttl' -delete
      # Replace the blank nodes with named nodes based on the filename. We add a validation: prefix
      - sed -e "s/_:genid_validate-/validation:/g" {{.TEMP_FOLDER}}/validation_output.nt > {{.TEMP_FOLDER}}/validation_output_temp.ttl
      # Add the PREFIX line to the top of the file
      - 'echo "@prefix validation: <http://data.skkg.ch/validation/> ." | cat - {{.TEMP_FOLDER}}/validation_output_temp.ttl > {{.TEMP_FOLDER}}/validation_output.ttl'
      # Remove in-between files
      - rm -f {{.TEMP_FOLDER}}/validation_output_temp.ttl
      - rm -f {{.TEMP_FOLDER}}/validation_output.nt
      # Combine individual reports into a single report by extracting the IRI of the report and replacing it with a single IRI
      - grep '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/ns/shacl#ValidationReport>' "{{.TEMP_FOLDER}}/validation_output.ttl" | awk '{print $1}' > {{.TEMP_FOLDER}}/iri_list.txt
      - cat "{{.TEMP_FOLDER}}/iri_list.txt"
      - |
        for iri in $(cat "{{.TEMP_FOLDER}}/iri_list.txt"); do
          sed -i "s|$iri|{{.REPORT_IRI}}|g" "{{.TEMP_FOLDER}}/validation_output.ttl"
        done
      - rm {{.TEMP_FOLDER}}/iri_list.txt
      - echo "{{.REPORT_IRI}} <http://www.w3.org/ns/prov#endedAtTime> \"$(date "+%Y-%m-%dT%H:%M:%S")\"^^<http://www.w3.org/2001/XMLSchema#dateTime> ." >> {{.TEMP_FOLDER}}/validation_output.ttl
      # Add a rdfs:label based on the source folder
      - echo "{{.REPORT_IRI}} <http://www.w3.org/2000/01/rdf-schema#label> \"Validation report {{.REPORT_SOURCEFOLDER}}\" ." >> {{.TEMP_FOLDER}}/validation_output.ttl
      # Ingest to graph 
      - "curl -X POST -H 'Content-Type: application/x-turtle'  --data-binary @{{.TEMP_FOLDER}}/validation_output.ttl {{.ENDPOINT}}?context-uri={{.REPORTS_GRAPH}}"
      - rm -f {{.TEMP_FOLDER}}/validation_output.ttl
      
  _download-vocabulary-from-museumplus:
    internal: true
    requires:
      vars: [VOCABULARY]
    vars:
      OUTPUT_FOLDER: /data/source/vocabularies
    cmds:
      - echo "Downloading {{.VOCABULARY}} vocabulary from MuseumPlus"
      - mkdir -p {{.OUTPUT_FOLDER}}
      - python /scripts/downloadVocabulary.py --url $MUSEUMPLUS_URL --username $MUSEUMPLUS_USERNAME --password $MUSEUMPLUS_PASSWORD --vocabulary {{.VOCABULARY}} --outputFolder {{.OUTPUT_FOLDER}}

  _clean_turtle_file_from_extra_prefixes:
    desc: Removes additional prefix statements in a Turtle file
    internal: true
    requires:
      vars: [FILE]
    vars:
      TEMP_FILE:
        sh: echo "/data/temp/$(basename {{.FILE}}).cleaned"
    cmds:
      - cp {{.FILE}} {{.TEMP_FILE}}
      - awk '/^@prefix/ && !seen[$0]++ { print }' {{.TEMP_FILE}} > {{.FILE}}
      - grep -v "^@prefix" {{.TEMP_FILE}} >> {{.FILE}}
      - rm -f {{.TEMP_FILE}}

  _download-module-items-from-museumplus:
    internal: true
    requires:
      vars: [MODULE]
    vars:
      FOLDER:
        sh: echo "/data/source/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      FILENAMEPREFIX:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print tolower($0)}')-item-"
    cmds:
      - echo "Downloading {{.MODULE}} items from MuseumPlus"
      - mkdir -p {{.FOLDER}}
      - task: _download-items-from-museumplus
        vars:
          MODULE: "{{.MODULE}}"
          OUTPUT_FOLDER: "{{.FOLDER}}"
          FILENAMEPREFIX: "{{.FILENAMEPREFIX}}"

  _download-items-from-museumplus:
    internal: true
    requires:
      vars: [MODULE, OUTPUT_FOLDER, FILENAMEPREFIX]
    vars:
      TEMP_FOLDER: /data/temp/download/temp_{{.MODULE}}
    interactive: True
    cmds:
      - mkdir -p {{.TEMP_FOLDER}}
      - python /scripts/downloadItems.py --url $MUSEUMPLUS_URL --username $MUSEUMPLUS_USERNAME --password $MUSEUMPLUS_PASSWORD --module {{.MODULE}} --outputFolder {{.OUTPUT_FOLDER}} --tempFolder {{.TEMP_FOLDER}} --filenamePrefix {{.FILENAMEPREFIX}} {{.CLI_ARGS}}

  _drop-graph:
    internal: true
    cmds:
      - curl --silent -X POST {{.BLAZEGRAPH_ENDPOINT}} --data-urlencode "update=DROP GRAPH <{{.GRAPH}}>" > /dev/null

  _generate-example-record:
    internal: true
    requires:
      vars: [OUTPUTFILE, INPUTFILES, MODULE]
    vars:
      OUTPUTFILE: "{{.OUTPUTFILE}}"
      INPUTFILES: "{{.INPUTFILES}}"
      MODULE: "{{.MODULE}}"
    cmds:
      - cat {{.INPUTFILES}} > {{.OUTPUTFILE}}
      - sed -i 's/<application xmlns="http:\/\/www.zetcom.com\/ria\/ws\/module">//g' {{.OUTPUTFILE}}
      - sed -i 's/<modules>//g' {{.OUTPUTFILE}}
      - sed -i 's/<\/modules>//g' {{.OUTPUTFILE}}
      - sed -i 's/<\/application>//g' {{.OUTPUTFILE}}
      - echo '<application><modules>' | cat - {{.OUTPUTFILE}} > temp && mv temp {{.OUTPUTFILE}}; echo '</modules></application>' >> {{.OUTPUTFILE}}
      - python /scripts/applyPreprocessingForExampleRecord.py --file {{.OUTPUTFILE}} --module {{.MODULE}}

  _ingest-data-from-file:
    internal: true
    vars:
      ENDPOINT: 
        sh: echo "{{if .ENDPOINT}}{{.ENDPOINT}}{{else}}{{.BLAZEGRAPH_ENDPOINT}}{{end}}"
    cmds:
      - echo "Ingest {{.NAME}}"
      - curl --silent -X POST {{.ENDPOINT}} --data-urlencode 'update={{if .GRAPH}}DROP GRAPH <{{.GRAPH}}>{{end}}'
      - curl -X POST -H 'Content-Type:{{.TYPE}}' --data-binary '@{{.FILE}}' {{.ENDPOINT}}{{if .GRAPH}}?context-uri={{.GRAPH}}{{end}}

  _ingest_module_items:
    internal: true
    requires:
      vars: [MODULE, NAMESPACE]
    vars:
      INPUTFOLDER:
        sh: echo "/data/ttl/main/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      TEMPFOLDER:
        sh: echo "/data/temp/ingest/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      XMLFOLDER:
        sh: echo "/data/source/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      FILENAMEPREFIX:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print tolower($0)}')-item-"
      NAMEDGRAPHBASE: "{{.NAMESPACE}}graph/"
      DEBUG:
        sh: |
          # Your input string of arguments
          args="{{.CLI_ARGS}}"
          # Check if "--debug" is present in the input string
          if [[ "$args" == *"--debug"* ]]; then
            # Use parameter expansion to extract the value after "--debug"
            value="${args#*--debug }"
            # Split the arguments by space and get the first part as the value
            value="${value%% *}"
            echo "$value"
          else
            if [ -z "{{.DEBUG}}" ]; then
              echo "false"
            else
              echo "{{.DEBUG}}"
            fi
          fi
    interactive: True
    cmds:
      - mkdir -p {{.TEMPFOLDER}}
      - |
        # If folder contains already ingested files we skip the preparation step
        if [ "$(find {{.TEMPFOLDER}} -type f -name '*.ttl.ingested' | wc -l)" -gt 0 ]; then
          echo "Skipping preparation step for {{.MODULE}} as there are already ingested files"
        else
          echo "Preparing {{.MODULE}} for ingest"
          python prepareDataForIngest.py  --inputFolder {{.INPUTFOLDER}} --outputFolder {{.TEMPFOLDER}} --xmlFolder {{.XMLFOLDER}}
        fi
      - |
        numfiles=$(find {{.TEMPFOLDER}} -type f -name '*.ttl' | wc -l)        
        count=1
        for f in $(find {{.TEMPFOLDER}} -type f -name '*.ttl' ); do
          identifier=$(basename "$f" | sed "s/{{.FILENAMEPREFIX}}//; s/\\.ttl//")
          graph="{{.NAMEDGRAPHBASE}}{{.FILENAMEPREFIX}}$identifier"
          echo "Ingesting {{.MODULE}} item $count of $numfiles into graph $graph"
          # Drop graph
          curl --silent -X POST {{.BLAZEGRAPH_ENDPOINT}} --data-urlencode "update=DROP GRAPH <$graph>" {{if eq .DEBUG "true"}}{{else}} > /dev/null{{end}}
          # Ingest into graph
          curl --silent -X POST --data-binary "uri=file://$f" "{{.BLAZEGRAPH_ENDPOINT}}?context-uri=$graph" {{if eq .DEBUG "true"}}{{else}} > /dev/null{{end}}
          count=$((count+1)) 
          mv $f $f.ingested
        done
      - task: _update-metadata-for-module
        vars:
          MODULE: "{{.MODULE}}"
          KEY: lastIngested
          VALUE:
            sh: date "+%Y-%m-%d %H:%M:%S.000"
          SOURCEFOLDER: "{{.TEMPFOLDER}}"
          FILEEXTENSION: .ttl.ingested
      - find {{.TEMPFOLDER}} -maxdepth 1 -name "*.ttl.ingested" -delete
  
  _ingest_vocabularies:
    internal: true
    requires:
      vars: [INPUTFOLDER, NAMESPACE]
    vars:
      NAMESPACE: "{{.NAMESPACE}}"
    cmds:
      - echo "Ingesting vocabularies"
      - |
        numfiles=$(find {{.INPUTFOLDER}} -type f -name '*.ttl' | wc -l)
        count=1
        for f in $(find {{.INPUTFOLDER}} -type f -name '*.ttl' ); do
          uri=$(grep -oPm 1 '(<{{.NAMESPACE}})[0-9]+' $f) 
          # Strip < at beginning and end of uri
            graph=${uri:1}
          echo -e "Ingesting vocabulary $graph ($count of $numfiles)"
          # Drop graph
          curl --silent -X POST {{.BLAZEGRAPH_ENDPOINT}} --data-urlencode "update=DROP GRAPH <$graph>" > /dev/null
          # Ingest vocabulary into graph
          curl --silent -X POST --data-binary "uri=file://$f" "{{.BLAZEGRAPH_ENDPOINT}}?context-uri=$graph"
          count=$((count+1)) 
        done

  _output_message:
    internal: true
    decs: Output a message to the console
    silent: true
    requires:
      vars: [MESSAGE]
    cmds:
      - echo "{{.MESSAGE}}"

  _perform-mapping-for-module-items:
    internal: true
    requires:
      vars: [MODULE]
    vars:
      INPUTFOLDER:
        sh: echo "/mapping/input/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      OUTPUTFOLDER:
        sh: echo "/mapping/output/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      DESTINATIONFOLDER:
        sh: echo "/data/ttl/main/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      MAPPINGFILE:
        sh: echo "/mapping/mapping-$(echo "{{.MODULE}}" | awk '{print tolower($0)}').x3ml"
    status:
      - '[ "$(find {{.INPUTFOLDER}} -name "*.xml" -type f)" ] || exit 0 && exit 1'
    cmds:
      - mkdir -p {{.OUTPUTFOLDER}}
      - find {{.OUTPUTFOLDER}} -maxdepth 1 -name "*.ttl" -delete
      - bash /scripts/performMapping.sh -i {{.INPUTFOLDER}} -o {{.OUTPUTFOLDER}} -m {{.MAPPINGFILE}} -g {{.GENERATOR_POLICY}} -j {{.X3ML_ENDPOINT}}
      - sleep 5s # Wait for the mapping to finish
      - task: _update-metadata-for-module
        vars:
          MODULE: "{{.MODULE}}"
          KEY: lastMapped
          VALUE:
            sh: date "+%Y-%m-%d %H:%M:%S.000"
          SOURCEFOLDER: "{{.INPUTFOLDER}}"
          FILEEXTENSION: .xml
      - find {{.INPUTFOLDER}} -maxdepth 1 -name "*.xml" -delete
      - task: _perform-post-mapping-tasks
        vars:
          MODULE: "{{.MODULE}}"
          FOLDER: "{{.OUTPUTFOLDER}}"
      - mkdir -p {{.DESTINATIONFOLDER}}
      - find {{.OUTPUTFOLDER}} -maxdepth 1 -name "*.ttl" -exec mv -t {{.DESTINATIONFOLDER}} {} +

  _perform-post-mapping-tasks:
    internal: true
    requires:
      vars: [MODULE, FOLDER]
    cmds:
      - echo "Performing post-mapping tasks for {{.MODULE}}"
      - task: _validate-turtle-files-in-folder
        vars:
          FOLDER: "{{.FOLDER}}"

  _prepare-mapping-for-module-items:
    requires:
      vars: [MODULE]
    vars:
      MODULE: "{{.MODULE}}"
      INPUTFOLDER:
        sh: echo "/data/source/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      OUTPUTFOLDER:
        sh: echo "/mapping/input/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
    cmds:
      - mkdir -p {{.OUTPUTFOLDER}}
      - find {{.OUTPUTFOLDER}} -maxdepth 1 -name "*.xml" -delete
      - python /scripts/prepareDataForMapping.py --module {{.MODULE}} --inputFolder {{.INPUTFOLDER}} --outputFolder {{.OUTPUTFOLDER}} {{.CLI_ARGS}}

  _process-items-unpublished-from-museumplus:
    internal: true
    vars: 
      NAMEDGRAPHBASE: "{{.NAMESPACE}}graph/"
    interactive: True
    cmds:
      - python /scripts/processUnpublishedItems.py --url $MUSEUMPLUS_URL --username $MUSEUMPLUS_USERNAME --password $MUSEUMPLUS_PASSWORD --module {{.MODULE}} --inputFolder {{.INPUT_FOLDER}} --turtleFolder {{.TURTLE_FOLDER}} --namedGraphBase {{.NAMEDGRAPHBASE}} --filenamePrefix {{.FILENAMEPREFIX}} --sparqlEndpoint {{.BLAZEGRAPH_ENDPOINT}}

  _remove-unpublished-module-items:
    internal: true
    requires:
      vars: [MODULE]
    vars:
      FILENAMEPREFIX:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print tolower($0)}')-item-"
      TURTLEFOLDER:
        sh: echo "/data/ttl/main/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      XMLFOLDER:
        sh: echo "/data/source/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
    cmds:
      - echo "Checking for unpublished items in {{.MODULE}}"
      - task: _process-items-unpublished-from-museumplus
        vars:
          MODULE: "{{.MODULE}}"
          INPUT_FOLDER: "{{.XMLFOLDER}}"
          TURTLE_FOLDER: "{{.TURTLEFOLDER}}"
          FILENAMEPREFIX: "{{.FILENAMEPREFIX}}"

  _remove-module-items-from-triplestore:
    internal: true
    requires:
      vars: [MODULE]
    vars:
      NAMEDGRAPHBASE: "{{.NAMESPACE}}graph/"
      ITEMPREFIX:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print tolower($0)}')-item-"
      QUERY: |
        PREFIX skkg: <https://ontology.skkg.ch/>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        SELECT DISTINCT ?g WHERE {
          GRAPH ?g {
            ?s a/rdfs:subClassOf* skkg:Entity
          }
          FILTER(STRSTARTS(STR(?g), \"{{.NAMEDGRAPHBASE}}{{.ITEMPREFIX}}\"))
        }
    cmds:
      - echo "Removing {{.MODULE}} items from the triplestore"
      - |
        MAX_RETRIES=5
        RESPONSE=$(curl --silent -X POST "{{.BLAZEGRAPH_ENDPOINT}}?format=json" --data-urlencode "query={{.QUERY}}")
        GRAPHS=$(echo $RESPONSE | jq -r '.results.bindings[].g.value')
        CURRENT_GRAPH_INDEX=1
        TOTAL_NUM_GRAPHS=$(echo $GRAPHS | wc -w)
        RETRY_COUNT=1
        for GRAPH in $GRAPHS; do
          DROP_QUERY="DROP GRAPH <$GRAPH>"
          echo "Dropping graph $CURRENT_GRAPH_INDEX/$TOTAL_NUM_GRAPHS ($GRAPH)"
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            curl --silent -X POST "{{.BLAZEGRAPH_ENDPOINT}}" --data-urlencode "update=$DROP_QUERY" > /dev/null || true
            if [ $? -eq 0 ]; then
              RETRY_COUNT=1
              CURRENT_GRAPH_INDEX=$((CURRENT_GRAPH_INDEX+1))
              break
            else
              echo "Retrying..."
              RETRY_COUNT=$((RETRY_COUNT+1))
              sleep 2
            fi
          done
        done

  _remove-module-items-without-equivalent-ttl-from-triplestore:
    internal: true
    requires:
      vars: [MODULE]
    silent: true
    vars:
      NAMEDGRAPHBASE: "{{.NAMESPACE}}graph/"
      ITEMPREFIX:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print tolower($0)}')-item-"
      TTLFOLDER:
        sh: echo "/data/ttl/main/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      QUERY: |
        PREFIX skkg: <https://ontology.skkg.ch/>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        SELECT DISTINCT ?g WHERE {
          GRAPH ?g {
            ?s a/rdfs:subClassOf* skkg:Entity
          }
          FILTER(STRSTARTS(STR(?g), \"{{.NAMEDGRAPHBASE}}{{.ITEMPREFIX}}\"))
        }
    cmds:
      - echo "Checking for {{.MODULE}} items in the triplestore that do not have an equivalent TTL file..."
      - |
        MAX_RETRIES=5
        RESPONSE=$(curl --silent -X POST "{{.BLAZEGRAPH_ENDPOINT}}?format=json" --data-urlencode "query={{.QUERY}}")
        GRAPHS=$(echo $RESPONSE | jq -r '.results.bindings[].g.value')
        TOTAL_NUM_GRAPHS=$(echo $GRAPHS | wc -w)
        echo "Total number of graphs: $TOTAL_NUM_GRAPHS"
        TTL_FILES=$(find {{.TTLFOLDER}} -type f -name "*.ttl" | xargs -n 1 basename | sed 's/.ttl//')
        TOTAL_NUM_TTL_FILES=$(echo $TTL_FILES | wc -w)
        echo "Total number of TTL files: $TOTAL_NUM_TTL_FILES"
        # For each GRAPH in the triplestore check if there is a corresponding TTL file. If not remove the GRAPH
        RETRY_COUNT=1
        for GRAPH in $GRAPHS; do
          # Extract the GRAPH name from the full URI by removing the NAMEDGRAPHBASE
          GRAPH_NAME="${GRAPH#{{.NAMEDGRAPHBASE}}}"
          if [[ $TTL_FILES == *$GRAPH_NAME* ]]; then
            continue
          else
            DROP_QUERY="DROP GRAPH <$GRAPH>"
            echo "Dropping graph $GRAPH"
            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              curl --silent -X POST "{{.BLAZEGRAPH_ENDPOINT}}" --data-urlencode "update=$DROP_QUERY" > /dev/null || true
              if [ $? -eq 0 ]; then
                RETRY_COUNT=1
                break
              else
                echo "Retrying..."
                RETRY_COUNT=$((RETRY_COUNT+1))
                sleep 2
              fi
            done
          fi
        done

  _reset-iiif-and-vocabularies:
    internal: true
    desc: Subtask to reset the IIIF and vocabularies data
    prompt: Do you also want to reset the IIIF and vocabularies data?
    cmds:
      - task: reset-vocabularies
      - task: reset-iiif

  _run-task-according-to-interval:
    internal: true
    desc: Run a task according to a specified interval
    requires:
      vars: [TASK, INTERVAL]
    deps:
      - task: _output_message
        vars:
          MESSAGE: "Checking if task '{{.TASK}}' has been run in the last {{.INTERVAL}}..."
    vars:
      LAST_RUN:
        sh: cat /scripts/.task/lastrun/{{.TASK}} 2>/dev/null || echo 0
      INTERVAL_IN_SECONDS:
        sh: |
          FREQUENCY="{{.INTERVAL}}"
          case "${FREQUENCY: -1}" in
            "s") INTERVAL=${FREQUENCY%?} ;;   # Seconds
            "m") INTERVAL=$((${FREQUENCY%?} * 60)) ;;   # Minutes
            "h") INTERVAL=$((${FREQUENCY%?} * 3600)) ;; # Hours
            "d") INTERVAL=$((${FREQUENCY%?} * 86400)) ;; # Days
            "w") INTERVAL=$((${FREQUENCY%?} * 604800)) ;; # Weeks
            "M") INTERVAL=$((${FREQUENCY%?} * 2592000)) ;; # Months (approx 30 days)
            "y") INTERVAL=$((${FREQUENCY%?} * 31536000)) ;; # Years (365 days)
            *)
              echo "Invalid frequency format."
              exit 1
              ;;
          esac
          echo $INTERVAL
      CURRENT_TIME: 
        sh: date +%s
    status:
      - |
          CURRENT_TIME={{.CURRENT_TIME}}
          LAST_RUN={{.LAST_RUN}}
          INTERVAL_IN_SECONDS={{.INTERVAL_IN_SECONDS}}
          if [ $(($CURRENT_TIME - $LAST_RUN)) -ge $INTERVAL_IN_SECONDS ]; then
            exit 1
          else
            exit 0
          fi
    cmds:
      - task: "{{.TASK}}"
      - mkdir -p /scripts/.task/lastrun
      - echo "{{.CURRENT_TIME}}" > /scripts/.task/lastrun/{{.TASK}}

  _suggest-alignments:
    internal: True
    interactive: True
    requires:
      vars: [INPUT_FILE, RECONCILIATION_TYPE, BASE_TYPE, RECONCILIATION_TYPE]
    status:
      - |
        # Compare the date of the input file with the date of the output file. if both have been created on the same day, we assume the task is up to date
        INPUT_FILE_DATE=$(date -r {{.INPUT_FILE}} "+%Y-%m-%d")
        OUTPUT_FILE_DATE=$(date -r {{.OUTPUTFILE}} "+%Y-%m-%d")
        if [ "$INPUT_FILE_DATE" == "$OUTPUT_FILE_DATE" ]; then
          exit 0
        else
          exit 1
        fi
    vars:
      LIMIT: 5
      OUTPUTFILE:
        sh: echo "/data/ttl/additional/classifications/$(basename {{.INPUT_FILE}} .ttl)-alignments.ttl"
    cmds:
        - python /scripts/suggestAlignments.py --input_file "{{.INPUT_FILE}}" --reconciliation_type "{{.RECONCILIATION_TYPE}}"  --limit "{{.LIMIT}}" --output_file "{{.OUTPUTFILE}}" --base_type "{{.BASE_TYPE}}" {{if .LOG_FILE}}--log_file "{{.LOG_FILE}}"{{end}}
        - # Set the date of the output file to the current date and time
        - touch -d "$(date "+%Y-%m-%d %H:%M:%S")" {{.OUTPUTFILE}}

  _update-metadata-for-module:
    internal: true
    requires:
      vars: [MODULE, KEY, VALUE, SOURCEFOLDER, FILEEXTENSION]
    vars:
      METADATAFOLDER:
        sh: echo "/data/source/$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
      SOURCEFOLDER: "{{.SOURCEFOLDER}}"
      KEY: "{{.KEY}}"
      VALUE: "{{.VALUE}}"
      FILEEXTENSION: "{{.FILEEXTENSION}}"
    cmds:
      - python /scripts/updateMetadataValueForFiles.py --metadataFolder {{.METADATAFOLDER}} --inputFolder {{.SOURCEFOLDER}} --key {{.KEY}} --value "{{.VALUE}}" --fileExtension {{.FILEEXTENSION}}

  _validate-turtle-files-in-folder:
    internal: true
    requires:
      vars: [FOLDER]
    vars:
      SHAPES_GRAPH: /mapping/shapesGraph.ttl
      SCHEMA_SKKG: /mapping/schemas/skkg-ontology.ttl
      SCHEMA_CRM: https://cidoc-crm.org/rdfs/7.1.1/CIDOC_CRM_v7.1.1.rdfs
      REPORTS_GRAPH: "{{.NAMESPACE}}graph/shaclReports"
      LIMIT: 50
      TEMP_FOLDER: /data/temp/validate
      FILE_PREFIX: validate-
      REPORT_IRI:
        sh: echo "validation:validation-report-$(echo {{.FOLDER}} | sed 's|/|-|g')-$(date "+%Y-%m-%d-%H-%M-%S")"
    interactive: True
    status:
      - '[ "$(find {{.FOLDER}} -name "*.ttl" -type f)" ] || exit 0 && exit 1'
    cmds:
      - mkdir -p {{.TEMP_FOLDER}}
      - find {{.TEMP_FOLDER}} -type f -name '*.ttl' -delete
      # Validate each file in the folder, outputting the failed tests to temporary turtle files
      - |
        numfiles=$(find {{.FOLDER}} -type f -name '*.ttl' | wc -l)
        count=1
        for f in $(find {{.FOLDER}} -type f -name '*.ttl' | head -n {{.LIMIT}}); do
          echo "Validating file $count of $numfiles: $f"
          pyshacl -s {{.SHAPES_GRAPH}} -e {{.SCHEMA_SKKG}} -e {{.SCHEMA_CRM}} -m -i rdfs -a -j -f turtle $f -o {{.TEMP_FOLDER}}/{{.FILE_PREFIX}}$(basename $f) || true
          # Delete the report output file if it does conform
          if grep -q 'sh:conforms true' {{.TEMP_FOLDER}}/{{.FILE_PREFIX}}$(basename $f); then
            rm {{.TEMP_FOLDER}}/{{.FILE_PREFIX}}$(basename $f)
          fi
          count=$((count+1))
          if [ $count -gt {{.LIMIT}} ]; then
            echo "Notice: Validation is limited to {{.LIMIT}} files. Only the first {{.LIMIT}} files have been validated."
            break
          fi
        done
      # If tests have failed there will be files in the temp folder, output a message to the console
      - |
        if [ "$(find {{.TEMP_FOLDER}} -type f -name '*.ttl' -size +0c | wc -l)" -gt 0 ]; then
          echo "Validation errors found. Check the SHACL reports in the triplestore."
          if [ -n "$SLACK_WEBHOOK_URL" ]; then
            curl -X POST -H 'Content-type: application/json' --data '{"text":"Validation errors found when mapping files in {{.FOLDER}}. Check the SHACL reports in the triplestore."}' "$SLACK_WEBHOOK_URL"
          fi
        else
          echo "Validation successful."
        fi
      - task: _combine_and_ingest_shacl_reports
        vars:
          TEMP_FOLDER: "{{.TEMP_FOLDER}}"
          REPORTS_GRAPH: "{{.REPORTS_GRAPH}}"
          REPORT_SOURCEFOLDER: "{{.FOLDER}}"
          REPORT_IRI: "{{.REPORT_IRI}}"
