# https://taskfile.dev

version: '3'

vars:
  OUTPUTFOLDER_DUMP: /data/ttl/dump
  OUTPUTFOLDER_CSV: /data/csv

tasks:
  create-data-dump:
    desc: Creates a TTL dump of the data generated by the pipeline
    vars:
      FILENAME:
        sh: echo dump-$(date "+%Y-%m-%d-%H-%M-%S")
    cmds:
      - mkdir -p {{.OUTPUTFOLDER_DUMP}}
      - rm -f {{.OUTPUTFOLDER_DUMP}}/*.ttl
      - find /data/ttl/main/ -name "*.ttl" -exec cat {} + > {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - task: _clean_turtle_file_from_extra_prefixes
        vars:
          FILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - gzip -c {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl > {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz
      - rm -f {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - # Remove old dumps if NUMBER_OF_DUMPS_TO_KEEP is set
      - if [ -n "$NUMBER_OF_DUMPS_TO_KEEP" ]; then
          ls -t {{.OUTPUTFOLDER_DUMP}}/dump-*.ttl.gz | tail -n +$(($NUMBER_OF_DUMPS_TO_KEEP+1)) | xargs rm -f;
        fi
      - echo "Data dump created at {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz"
      
  create-data-dumps:
    desc: Create a full and partial TTL dump of the data generated by the pipeline, as well as the multimedia filename CSV
    cmds:
      - task: create-data-dump
      - task: create-partial-data-dump
      - task: generate-multimedia-filename-csv

  create-partial-data-dump:
    desc: Creates a partial TTL dump of the data generated by the pipeline
    vars:
      FILENAME: 
        sh: echo partial-dump-$(date "+%Y-%m-%d-%H-%M-%S")
      NUMFILES: 1000
      OBJECT_UUID_QUERY: |
        PREFIX la: <https://linked.art/ns/terms/>
        PREFIX skkg: <https://ontology.skkg.ch/>
        SELECT ?uuid WHERE {
          <https://data.skkg.ch/objectgroup/07115c7c-a3db-4c70-828d-2a3699416254> la:has_member ?object .
          ?object a skkg:Object .
          BIND(STRAFTER(STR(?object), "/object/") as ?uuid) 
        }
      PERSON_UUID_QUERY: |
        PREFIX la: <https://linked.art/ns/terms/>
        PREFIX skkg: <https://ontology.skkg.ch/>
        PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>
        SELECT ?uuid WHERE {
          <https://data.skkg.ch/objectgroup/07115c7c-a3db-4c70-828d-2a3699416254> la:has_member ?object .
          ?object a skkg:Object ;
            crm:P108i_was_produced_by/crm:P9_consists_of/crm:P14_carried_out_by ?person .
          BIND(STRAFTER(STR(?person), "/person/") as ?uuid) 
        }
      MULTIMEDIA_UUID_QUERY: |
        PREFIX la: <https://linked.art/ns/terms/>
        PREFIX skkg: <https://ontology.skkg.ch/>
        PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>
        SELECT ?uuid WHERE {
          <https://data.skkg.ch/objectgroup/07115c7c-a3db-4c70-828d-2a3699416254> la:has_member ?object .
          ?object a skkg:Object ;
            crm:P138i_has_representation ?multimedia .
          ?multimedia a skkg:Multimedia ;
          BIND(STRAFTER(STR(?multimedia), "/multimedia/") as ?uuid) 
        }
    cmds:
      - mkdir -p {{.OUTPUTFOLDER_DUMP}}
      - rm -f {{.OUTPUTFOLDER_DUMP}}/*.ttl
      # - |
      #   # Retrieve NUMFILES number of files from each directory in /data/ttl/main/
      #   find /data/ttl/main/ -type d | while read dir; do
      #     find "$dir" -maxdepth 1 -name "*.ttl" | head -n {{.NUMFILES}} | xargs cat >> {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      #   done
      - |
        # Retrieve UUIDs and store them in an array
        mapfile -t UUIDS < <(curl -sG '{{.BLAZEGRAPH_ENDPOINT}}' \
          --data-urlencode 'query={{.OBJECT_UUID_QUERY}}' \
          -H 'Accept: text/csv' | tail -n +2)

        TOTAL=${#UUIDS[@]}
        COUNT=1

        for uuid in "${UUIDS[@]}"; do
            CLEAN_UUID=$(echo "$uuid" | tr -d '\r')
            FILE_PATH="/data/ttl/main/object/object-item-${CLEAN_UUID}.ttl"
            
            echo "Processing file $COUNT/$TOTAL: $FILE_PATH"
            
            if [ -f "$FILE_PATH" ]; then
                cat "$FILE_PATH" >> {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
            else
                echo "File $FILE_PATH does not exist" >&2
            fi
            
            ((COUNT++))
        done
      - |
        # Retrieve the UUIDs of the persons in the specified group 
        curl -sG '{{.BLAZEGRAPH_ENDPOINT}}' \
          --data-urlencode 'query={{.PERSON_UUID_QUERY}}' \
                  -H 'Accept: text/csv' | tail -n +2 | \
        while read uuid; do
            FILE_PATH="/data/ttl/main/person/person-item-$(echo "$uuid" | tr -d '\r').ttl"
            if [ -f "$FILE_PATH" ]; then
              cat "$FILE_PATH" >> {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
            else
              echo "File $FILE_PATH does not exist" >&2
            fi
        done
      - |
        # Retrieve the UUIDs of the multimedia items in the specified group 
        curl -sG '{{.BLAZEGRAPH_ENDPOINT}}' \
          --data-urlencode 'query={{.MULTIMEDIA_UUID_QUERY}}' \
                  -H 'Accept: text/csv' | tail -n +2 | \
        while read uuid; do
            FILE_PATH="/data/ttl/main/multimedia/multimedia-item-$(echo "$uuid" | tr -d '\r').ttl"
            if [ -f "$FILE_PATH" ]; then
              cat "$FILE_PATH" >> {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
            else
              echo "File $FILE_PATH does not exist" >&2
            fi
        done
      - task: _clean_turtle_file_from_extra_prefixes
        vars:
          FILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - gzip -c {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl > {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz
      - rm -f {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - echo "Partial data dump created at {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz" 

  generate-multimedia-filename-csv:
    desc: Generate a CSV file with Multimedia Item UUIDs and their corresponding filenames for use in the IIIF Image Server pipeline
    vars:
      MULTIMEDIAFOLDER: /data/source/multimedia
      OBJECTFOLDER: /data/source/object
      OUTPUTFILE: 
        sh: echo "{{.OUTPUTFOLDER_CSV}}/multimedia-filenames.csv"
    cmds:
      - python /scripts/generateMultimediaFilenameCSV.py --multimediaFolder {{.MULTIMEDIAFOLDER}} --objectsFolder {{.OBJECTFOLDER}} --outputFile {{.OUTPUTFILE}}
  
  push-latest-data-dump:
    desc: Upload latest data dump to S3 endpoint for data sharing
    vars:
      FILEPATH:
        sh: echo "$(ls -t {{.OUTPUTFOLDER_DUMP}}/dump*.ttl.gz | head -1)"
      FILENAME:
        sh: echo "$(basename {{.FILEPATH}})"
      LATEST_FILENAME: latest-dump.ttl.gz
    cmds:
      - defer: python /scripts/prometheusReporter.py --name skkg_s3_dump_upload_success --type gauge --action set --value {{if .EXIT_CODE}}{{.EXIT_CODE}}{{else}}0{{end}} --labels dump_type=full
      - echo "Uploading {{.FILEPATH}} to S3 bucket $S3_BUCKET_PUSH..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.FILENAME}}
      - echo "Uploading {{.FILEPATH}} as latest dump..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.LATEST_FILENAME}}

  push-latest-data-dumps:
    desc: Upload full and partial data dumps and CSV files to S3 endpoint
    cmds:
      - task: push-latest-data-dump
      - task: push-latest-partial-data-dump
      - task: push-multimedia-filename-csv
      
  push-latest-partial-data-dump:
    desc: Upload latest partial data dump to S3 endpoint for data sharing
    vars:
      FILEPATH:
        sh: echo "$(ls -t {{.OUTPUTFOLDER_DUMP}}/partial-dump*.ttl.gz | head -1)"
      FILENAME:
        sh: echo "$(basename {{.FILEPATH}})"
      LATEST_FILENAME: latest-partial-dump.ttl.gz
    cmds:
      - defer: python /scripts/prometheusReporter.py --name skkg_s3_dump_upload_success --type gauge --action set --value {{if .EXIT_CODE}}{{.EXIT_CODE}}{{else}}0{{end}} --labels dump_type=partial
      - echo "Uploading {{.FILEPATH}} to S3 bucket $S3_BUCKET_PUSH..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.FILENAME}}
      - echo "Uploading {{.FILEPATH}} as latest dump..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.LATEST_FILENAME}}
  
  push-multimedia-filename-csv:
    desc: Upload Multimedia filename CSV to S3 endpoint for data sharing
    vars:
      FILEPATH: 
        sh: echo "{{.OUTPUTFOLDER_CSV}}/multimedia-filenames.csv"
    cmds:
      - echo "Uploading {{.FILEPATH}} to S3 bucket $S3_BUCKET_PUSH..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.FILENAME}}

  run-pipeline-cycles:
    desc: Runs the entire pipeline as well as certain tasks according to a specific interval. When this task is executed, each step of the pipeline will be run if the interval has passed since the last execution of the task. The interval is defined in the vars section of each task.
    cmds:
      - task: _run-task-according-to-interval
        vars:
          TASK: default
          INTERVAL: 1d
      - task:  _run-task-according-to-interval
        vars:
          TASK: update-vocabularies
          INTERVAL: 1w
      - task:  _run-task-according-to-interval
        vars:
          TASK: update-iiif
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: remove-unpublished-items
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: remove-items-without-equivalent-ttl-from-triplestore
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: create-data-dumps
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: push-latest-data-dumps
          INTERVAL: 1w
  
  show-last-pipeline-cycle-runs:
    desc: Displays the last pipeline cycle runs for each task
    cmds:
      - |
        DIRECTORY="/scripts/.task/lastrun"

        printf "%-30s %-20s\n" "Task" "Last Run"
        printf "%-30s %-20s\n" "----------------" "-----------------"

        for FILE in "$DIRECTORY"/*; do
          # Get the base name of the file (without the directory path)
          BASENAME=$(basename "$FILE")

          # Extract the timestamp from the file (adjust this command based on file content)
          TIMESTAMP=$(cat "$FILE" | grep -oE '[0-9]{10}' | head -n 1)
          
          if [[ -n "$TIMESTAMP" ]]; then
            # Convert the timestamp to a human-readable date
            HUMAN_DATE=$(date -d @"$TIMESTAMP" +"%Y-%m-%d %H:%M:%S")
            printf "%-30s %-20s\n" "$BASENAME" "$HUMAN_DATE"
          else
            printf "%-30s %-20s\n" "$BASENAME" "No valid timestamp"
          fi
        done