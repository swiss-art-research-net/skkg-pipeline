# https://taskfile.dev

version: '3'

vars:
  OUTPUTFOLDER_DUMP: /data/ttl/dump
  OUTPUTFOLDER_CSV: /data/csv

tasks:
  create-data-dump:
    desc: Creates a TTL dump of the data generated by the pipeline
    vars:
      FILENAME:
        sh: echo dump-$(date "+%Y-%m-%d-%H-%M-%S")
    cmds:
      - mkdir -p {{.OUTPUTFOLDER_DUMP}}
      - rm -f {{.OUTPUTFOLDER_DUMP}}/*.ttl
      - # Add main data
      - find /data/ttl/main/ -name "*.ttl" -exec cat {} + > {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - # Add external data
      - find /data/ttl/additional/external/ -name "*.ttl" -exec cat {} + >> {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - # Add alignments
      - find /data/ttl/additional/alignments/ -name "*.ttl" -exec cat {} + >> {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - # Add .ttl ontologies
      - find /mapping/schemas -name "*.ttl" -exec cat {} + >> {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - # Add .rdf and .rdfs ontologies converted to .ttl 
      - find /mapping/schemas -name '*.rdf' -exec sh -c 'rapper -q -i rdfxml -o turtle "$1" >> "$2"' _ {} {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl \;
      - find /mapping/schemas -name '*.rdfs' -exec sh -c 'rapper -q -i rdfxml -o turtle "$1" >> "$2"' _ {} {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl \;
      - task: _clean_turtle_file_from_extra_prefixes
        vars:
          FILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - # Remove wrong prefix defind in LT external data
      - 'sed -i "/^@prefix owl: *<http:\/\/www.w3.org\/TR\/owl-semantics\/> *\./d" {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl'
      - gzip -c {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl > {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz
      - rm -f {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - # Remove old dumps if NUMBER_OF_DUMPS_TO_KEEP is set
      - if [ -n "$NUMBER_OF_DUMPS_TO_KEEP" ]; then
          ls -t {{.OUTPUTFOLDER_DUMP}}/dump-*.ttl.gz | tail -n +$(($NUMBER_OF_DUMPS_TO_KEEP+1)) | xargs rm -f;
        fi
      - echo "Data dump created at {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz"
      
  create-data-dumps:
    desc: Create a full and partial TTL dump of the data generated by the pipeline, as well as the multimedia filename CSV
    cmds:
      - task: create-data-dump
      - task: create-partial-data-dump
      - task: generate-multimedia-filename-csv

  create-partial-data-dump:
    desc: Creates a partial TTL dump of the data generated by the pipeline
    vars:
      FILENAME: 
        sh: echo partial-dump-$(date "+%Y-%m-%d-%H-%M-%S")
      NUMFILES: 1000
      ADDRESS_UUID_QUERY: |
        PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>
        PREFIX la: <https://linked.art/ns/terms/>
        PREFIX skkg: <https://ontology.skkg.ch/>
        SELECT DISTINCT ?uuid WHERE {
          <https://data.skkg.ch/objectgroup/07115c7c-a3db-4c70-828d-2a3699416254> la:has_member ?object .
          ?object a skkg:Object ;
            crm:P24i_changed_ownership_through/crm:P23_transferred_title_from? ?address .
          ?address a skkg:Address .
          FILTER(?address != <https://data.skkg.ch/address/privateIndividual>)
          BIND(STRAFTER(STR(?address), "/address/") as ?uuid) 
        }
      LITERATURE_UUID_QUERY: |
        PREFIX la: <https://linked.art/ns/terms/>
        PREFIX skkg: <https://ontology.skkg.ch/>
        PREFIX exhibition: <https://data.skkg.ch/exhibition/>
        PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>
        SELECT DISTINCT ?uuid WHERE {
          # Literature about specific exhibitions
          VALUES ?exhibition { exhibition:136 exhibition:152 exhibition:158 exhibition:1d42c3a2-a3b5-439e-b43f-0bc76f203a70 exhibition:2cf13c63-4233-4563-b8f3-839dd2df2c15 exhibition:492ee8a1-1dd0-4821-8189-e7329724f0b3 exhibition:8194e0bb-e9d8-4f12-a8fc-50fe5e453971 exhibition:9886763b-a3bb-4fab-98f3-a4164f40588a }
          ?literature a skkg:Literature ;
            crm:P67_refers_to ?exhibition .
          BIND(STRAFTER(STR(?literature), "/literature/") as ?uuid) 
        }
      MULTIMEDIA_UUID_QUERY: |
        PREFIX la: <https://linked.art/ns/terms/>
        PREFIX skkg: <https://ontology.skkg.ch/>
        PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>
        SELECT DISTINCT ?uuid WHERE {
          <https://data.skkg.ch/objectgroup/07115c7c-a3db-4c70-828d-2a3699416254> la:has_member ?object .
          ?object a skkg:Object .
          {
            ?object crm:P138i_has_representation ?multimedia .
          } UNION {
            ?exhibition crm:P16_used_specific_object/la:has_member ?object ;
              crm:P67i_is_referred_to_by ?multimedia .
          }
          ?multimedia a skkg:Multimedia .
          BIND(STRAFTER(STR(?multimedia), "/multimedia/") as ?uuid) 
        }
      OBJECT_UUID_QUERY: |
        PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>
        PREFIX la: <https://linked.art/ns/terms/>
        PREFIX skkg: <https://ontology.skkg.ch/>
        PREFIX exhibition: <https://data.skkg.ch/exhibition/>
        SELECT DISTINCT ?uuid WHERE {
          {
            # Objects contained in group Sammlung Digital Test
            <https://data.skkg.ch/objectgroup/07115c7c-a3db-4c70-828d-2a3699416254> la:has_member ?object .
          } UNION {
            # Objects exhibited in specific exhibitions
            VALUES ?exhibition { exhibition:136 exhibition:152 exhibition:158 exhibition:1d42c3a2-a3b5-439e-b43f-0bc76f203a70 exhibition:2cf13c63-4233-4563-b8f3-839dd2df2c15 exhibition:492ee8a1-1dd0-4821-8189-e7329724f0b3 exhibition:8194e0bb-e9d8-4f12-a8fc-50fe5e453971 exhibition:9886763b-a3bb-4fab-98f3-a4164f40588a }
            ?exhibition crm:P16_used_specific_object/la:has_member ?object
          } UNION {
            # Objects referenced in literature about specific exhibitions
            VALUES ?exhibition { exhibition:136 exhibition:152 exhibition:158 exhibition:1d42c3a2-a3b5-439e-b43f-0bc76f203a70 exhibition:2cf13c63-4233-4563-b8f3-839dd2df2c15 exhibition:492ee8a1-1dd0-4821-8189-e7329724f0b3 exhibition:8194e0bb-e9d8-4f12-a8fc-50fe5e453971 exhibition:9886763b-a3bb-4fab-98f3-a4164f40588a }
            ?literature a skkg:Literature ;
              crm:P67_refers_to ?exhibition ;
              crm:P165_incorporates/crm:P67_refers_to ?object .
          }
          ?object a skkg:Object .
          BIND(STRAFTER(STR(?object), "/object/") as ?uuid) 
        }
      OWNERSHIP_UUID_QUERY: |
        PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>
        PREFIX la: <https://linked.art/ns/terms/>
        PREFIX skkg: <https://ontology.skkg.ch/>
        PREFIX exhibition: <https://data.skkg.ch/exhibition/>
        SELECT DISTINCT ?uuid WHERE {
          # Objects contained in group Sammlung Digital Test
          <https://data.skkg.ch/objectgroup/07115c7c-a3db-4c70-828d-2a3699416254> la:has_member ?object .
          ?ownership a skkg:Provenance ;
            crm:P9_consists_of/crm:P16_used_specific_object ?object .
          BIND(STRAFTER(STR(?ownership), "/provenance/") as ?uuid) 
        }
      PERSON_UUID_QUERY: |
        PREFIX crm: <http://www.cidoc-crm.org/cidoc-crm/>
        PREFIX exhibition: <https://data.skkg.ch/exhibition/>
        PREFIX la: <https://linked.art/ns/terms/>
        PREFIX skkg: <https://ontology.skkg.ch/>
        SELECT DISTINCT ?uuid WHERE {
          {
            <https://data.skkg.ch/objectgroup/07115c7c-a3db-4c70-828d-2a3699416254> la:has_member ?object .
          } UNION {
            # Objects referenced in literature about specific exhibitions
            VALUES ?exhibition { exhibition:136 exhibition:152 exhibition:158 exhibition:1d42c3a2-a3b5-439e-b43f-0bc76f203a70 exhibition:2cf13c63-4233-4563-b8f3-839dd2df2c15 exhibition:492ee8a1-1dd0-4821-8189-e7329724f0b3 exhibition:8194e0bb-e9d8-4f12-a8fc-50fe5e453971 exhibition:9886763b-a3bb-4fab-98f3-a4164f40588a }
            ?literature a skkg:Literature ;
              crm:P67_refers_to ?exhibition ;
              crm:P165_incorporates/crm:P67_refers_to ?object .
          }
          ?object a skkg:Object .
          {
            ?object crm:P108i_was_produced_by/crm:P9_consists_of/crm:P14_carried_out_by|crm:P15_was_influenced_by ?person .
          } UNION {
            ?object crm:P138i_has_representation/crm:P104_is_subject_to/crm:P75i_is_possessed_by ?person .
          } UNION {
            ?object crm:P138i_has_representation/crm:P94i_was_created_by/crm:P14_carried_out_by ?person .
          } UNION {
            ?provenance a skkg:Provenance ;
              crm:P9_consists_of ?activity .
          ?activity crm:P16_used_specific_object ?object ;
            crm:P11_had_participant/crm:P107_has_current_or_former_member ?person .
          }	
          BIND(STRAFTER(STR(?person), "/person/") as ?uuid) 
        }
    cmds:
      - mkdir -p {{.OUTPUTFOLDER_DUMP}}
      - rm -f {{.OUTPUTFOLDER_DUMP}}/*.ttl
      - |
        # Retrieve NUMFILES number of files from each directory in /data/ttl/main/
        find /data/ttl/main/ -type d | while read dir; do
          find "$dir" -maxdepth 1 -name "*.ttl" | head -n {{.NUMFILES}} | xargs cat >> {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
        done
      - task: _combine_ttl_files_based_on_query
        vars:
          MODULE: address
          QUERY: "{{.ADDRESS_UUID_QUERY}}"
          OUTPUTFILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - task: _combine_ttl_files_based_on_query
        vars:
          MODULE: literature
          QUERY: "{{.LITERATURE_UUID_QUERY}}"
          OUTPUTFILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - task: _combine_ttl_files_based_on_query
        vars:
          MODULE: multimedia
          QUERY: "{{.MULTIMEDIA_UUID_QUERY}}"
          OUTPUTFILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - task: _combine_ttl_files_based_on_query
        vars:
          MODULE: object
          QUERY: "{{.OBJECT_UUID_QUERY}}"
          OUTPUTFILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - task: _combine_ttl_files_based_on_query
        vars:
          MODULE: ownership
          QUERY: "{{.OWNERSHIP_UUID_QUERY}}"
          OUTPUTFILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - task: _combine_ttl_files_based_on_query
        vars:
          MODULE: person
          QUERY: "{{.PERSON_UUID_QUERY}}"
          OUTPUTFILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - task: _clean_turtle_with_rapper
        vars:
          FILE: "{{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl"
      - gzip -c {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl > {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz
      - rm -f {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl
      - # Remove old dumps if NUMBER_OF_DUMPS_TO_KEEP is set
      - if [ -n "$NUMBER_OF_DUMPS_TO_KEEP" ]; then
          ls -t {{.OUTPUTFOLDER_DUMP}}/partial-dump-*.ttl.gz | tail -n +$(($NUMBER_OF_DUMPS_TO_KEEP+1)) | xargs rm -f;
        fi
      - echo "Partial data dump created at {{.OUTPUTFOLDER_DUMP}}/{{.FILENAME}}.ttl.gz" 

  generate-multimedia-filename-csv:
    desc: Generate a CSV file with Multimedia Item UUIDs and their corresponding filenames for use in the IIIF Image Server pipeline
    vars:
      MULTIMEDIAFOLDER: /data/source/multimedia
      OBJECTFOLDER: /data/source/object
      OUTPUTFILE: 
        sh: echo "{{.OUTPUTFOLDER_CSV}}/multimedia-filenames.csv"
    cmds:
      - python /scripts/generateMultimediaFilenameCSV.py --multimediaFolder {{.MULTIMEDIAFOLDER}} --objectsFolder {{.OBJECTFOLDER}} --outputFile {{.OUTPUTFILE}}
  
  push-latest-data-dump:
    desc: Upload latest data dump to S3 endpoint for data sharing
    vars:
      FILEPATH:
        sh: echo "$(ls -t {{.OUTPUTFOLDER_DUMP}}/dump*.ttl.gz | head -1)"
      FILENAME:
        sh: echo "$(basename {{.FILEPATH}})"
      LATEST_FILENAME: latest-dump.ttl.gz
    cmds:
      - defer: python /scripts/prometheusReporter.py --name skkg_s3_dump_upload_success --type gauge --action set --value {{if .EXIT_CODE}}{{.EXIT_CODE}}{{else}}0{{end}} --labels dump_type=full
      - echo "Uploading {{.FILEPATH}} to S3 bucket $S3_BUCKET_PUSH..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.FILENAME}}
      - echo "Uploading {{.FILEPATH}} as latest dump..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.LATEST_FILENAME}}

  push-latest-data-dumps:
    desc: Upload full and partial data dumps and CSV files to S3 endpoint
    cmds:
      - task: push-latest-data-dump
      - task: push-latest-partial-data-dump
      - task: push-multimedia-filename-csv
      
  push-latest-partial-data-dump:
    desc: Upload latest partial data dump to S3 endpoint for data sharing
    vars:
      FILEPATH:
        sh: echo "$(ls -t {{.OUTPUTFOLDER_DUMP}}/partial-dump*.ttl.gz | head -1)"
      FILENAME:
        sh: echo "$(basename {{.FILEPATH}})"
      LATEST_FILENAME: latest-partial-dump.ttl.gz
    cmds:
      - defer: python /scripts/prometheusReporter.py --name skkg_s3_dump_upload_success --type gauge --action set --value {{if .EXIT_CODE}}{{.EXIT_CODE}}{{else}}0{{end}} --labels dump_type=partial
      - echo "Uploading {{.FILEPATH}} to S3 bucket $S3_BUCKET_PUSH..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.FILENAME}}
      - echo "Uploading {{.FILEPATH}} as latest dump..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.LATEST_FILENAME}}
  
  push-multimedia-filename-csv:
    desc: Upload Multimedia filename CSV to S3 endpoint for data sharing
    vars:
      FILEPATH: 
        sh: echo "{{.OUTPUTFOLDER_CSV}}/multimedia-filenames.csv"
    cmds:
      - echo "Uploading {{.FILEPATH}} to S3 bucket $S3_BUCKET_PUSH..."
      - aws s3 cp "{{.FILEPATH}}" s3://$S3_BUCKET_PUSH/{{.FILENAME}}

  run-pipeline-cycles:
    desc: Runs the entire pipeline as well as certain tasks according to a specific interval. When this task is executed, each step of the pipeline will be run if the interval has passed since the last execution of the task. The interval is defined in the vars section of each task.
    cmds:
      - task: _run-task-according-to-interval
        vars:
          TASK: default
          INTERVAL: 1d
      - task:  _run-task-according-to-interval
        vars:
          TASK: update-vocabularies
          INTERVAL: 1w
      - task:  _run-task-according-to-interval
        vars:
          TASK: update-iiif
          INTERVAL: 2d
      - task: _run-task-according-to-interval
        vars:
          TASK: remove-unpublished-items
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: remove-items-without-equivalent-ttl-from-triplestore
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: create-data-dumps
          INTERVAL: 1w
      - task: _run-task-according-to-interval
        vars:
          TASK: push-latest-data-dumps
          INTERVAL: 1w
  
  show-last-pipeline-cycle-runs:
    desc: Displays the last pipeline cycle runs for each task
    cmds:
      - |
        DIRECTORY="/scripts/.task/lastrun"

        printf "%-30s %-20s\n" "Task" "Last Run"
        printf "%-30s %-20s\n" "----------------" "-----------------"

        for FILE in "$DIRECTORY"/*; do
          # Get the base name of the file (without the directory path)
          BASENAME=$(basename "$FILE")

          # Extract the timestamp from the file (adjust this command based on file content)
          TIMESTAMP=$(cat "$FILE" | grep -oE '[0-9]{10}' | head -n 1)
          
          if [[ -n "$TIMESTAMP" ]]; then
            # Convert the timestamp to a human-readable date
            HUMAN_DATE=$(date -d @"$TIMESTAMP" +"%Y-%m-%d %H:%M:%S")
            printf "%-30s %-20s\n" "$BASENAME" "$HUMAN_DATE"
          else
            printf "%-30s %-20s\n" "$BASENAME" "No valid timestamp"
          fi
        done
        

  _combine_ttl_files_based_on_query:
    desc: Combine TTL files based on a SPARQL query
    internal: true
    requires:
      vars: [MODULE, QUERY, OUTPUTFILE]
    vars:
      MODULE: 
        sh: echo "{{.MODULE}}"
      MODULE_UCFIRST:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print toupper(substr($0,0,1)) tolower(substr($0,2))}')"
      MODULE_LOWERCASE:
        sh: echo "$(echo "{{.MODULE}}" | awk '{print tolower($0)}')"
    cmds: 
      - |
        mapfile -t UUIDS < <(curl -sG '{{.BLAZEGRAPH_ENDPOINT}}' \
        --data-urlencode 'query={{.QUERY}}' \
        -H 'Accept: text/csv' | tail -n +2)

        TOTAL=${#UUIDS[@]}
        COUNT=1

        if [ ${#UUIDS[@]} -eq 0 ]; then
          echo "No UUIDs retrieved from the query." >&2
          exit 1
        fi

        for uuid in "${UUIDS[@]}"; do
            CLEAN_UUID=$(echo "$uuid" | tr -d '\r')
            FILE_PATH="/data/ttl/main/{{.MODULE_LOWERCASE}}/{{.MODULE_LOWERCASE}}-item-${CLEAN_UUID}.ttl"
            
            if [ -f "$FILE_PATH" ]; then
                cat "$FILE_PATH" >> {{.OUTPUTFILE}}
            else
                echo "File $FILE_PATH does not exist" >&2
            fi
            
            ((COUNT++))
        done
        echo "Added  $((COUNT-1)) files for module {{.MODULE}}. Output written to {{.OUTPUTFILE}}"